{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Substantailly copied by andrew@calcbench.com  \n",
    "# from http://scikit-learn.org/stable/auto_examples/text/document_clustering.html\n",
    "# Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>\n",
    "#         Lars Buitinck\n",
    "# http://scikit-learn.org/stable/auto_examples/text/document_clustering.html\n",
    "# License: BSD 3 clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calcbench as cb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://scikit-learn.org/stable/auto_examples/text/document_clustering.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumberNormalizingVectorizer(sklearn.feature_extraction.text.TfidfVectorizer):\n",
    "    def build_tokenizer(self):\n",
    "        tokenize = super(NumberNormalizingVectorizer, self).build_tokenizer()\n",
    "        return lambda doc: list(number_normalizer(tokenize(doc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_normalizer(tokens):\n",
    "    \"\"\" Map all numeric tokens to a placeholder.\n",
    "\n",
    "    For many applications, tokens that begin with a number are not directly\n",
    "    useful, but the fact that such a token exists can be relevant.  By applying\n",
    "    this form of dimensionality reduction, some methods may perform better.\n",
    "    \"\"\"\n",
    "\n",
    "    return (\"#NUMBER\" if token[0].isdigit() else token for token in tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The four biggest CIKs.\n",
    "industries = {\n",
    "    'pharmaceutical' : {'cik' : 2834},               \n",
    "    'REIT' : {'cik' : 6798},    \n",
    "    'software' : {'cik' : 7372}, \n",
    "    'oil' : {'cik' : 1311}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tickers = []\n",
    "CIK_map = {}\n",
    "for industry, industry_stuff in industries.items():\n",
    "    cik = industry_stuff['cik']\n",
    "    tickers = cb.tickers(SIC_codes=[cik])\n",
    "    all_tickers.extend(tickers)\n",
    "    CIK_map.update({ticker : cik for ticker in tickers})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = list(cb.document_search(document_name='Risk Factors', year=2016, company_identifiers=all_tickers))\n",
    "doc_contents = [BeautifulSoup(d.get_contents(), 'lxml').text for d in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1094 documents\n",
      "4 categories\n",
      "\n",
      "Extracting features from the training dataset using a sparse vectorizer\n",
      "done in 15.517489s\n",
      "n_samples: 1094, n_features: 1000\n",
      "\n",
      "Clustering sparse data with KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
      "    n_clusters=4, n_init=1, n_jobs=1, precompute_distances='auto',\n",
      "    random_state=None, tol=0.0001, verbose=True)\n",
      "Initialization complete\n",
      "Iteration  0, inertia 959.739\n",
      "Iteration  1, inertia 530.678\n",
      "Iteration  2, inertia 522.164\n",
      "Iteration  3, inertia 521.471\n",
      "Iteration  4, inertia 520.565\n",
      "Iteration  5, inertia 504.685\n",
      "Iteration  6, inertia 504.679\n",
      "Converged at iteration 6: center shift 0.000000e+00 within tolerance 8.123865e-08\n",
      "done in 1.847s\n",
      "\n",
      "Homogeneity: 0.794\n",
      "Completeness: 0.778\n",
      "V-measure: 0.786\n",
      "Adjusted Rand-Index: 0.757\n",
      "Silhouette Coefficient: 0.318\n",
      "\n",
      "Top terms per cluster:\n",
      "Cluster 0: gas oil reserves drilling wells fracturing exploration hydraulic proved emissions\n",
      "Cluster 1: clinical candidates trials fda patent drug patents manufacturing trial commercialization\n",
      "Cluster 2: smaller software solutions penny patent patents privacy broker platform acceptance\n",
      "Cluster 3: reit real distributions estate tenants lease mortgage partnership leases loans\n"
     ]
    }
   ],
   "source": [
    "print(\"%d documents\" % len(docs))\n",
    "print(\"%d categories\" % len(industries.keys()))\n",
    "print()\n",
    "\n",
    "labels = [CIK_map[d['ticker']] for d in docs]\n",
    "true_k = np.unique(labels).shape[0]\n",
    "\n",
    "print(\"Extracting features from the training dataset using a sparse vectorizer\")\n",
    "t0 = time()\n",
    "max_features = 1000 # From the original\n",
    "use_idf = False\n",
    "vectorizer = NumberNormalizingVectorizer(max_df=0.5, max_features=max_features,\n",
    "                                 min_df=2, stop_words='english',\n",
    "                                 use_idf=use_idf)\n",
    "\n",
    "X = vectorizer.fit_transform(doc_contents)\n",
    "\n",
    "print(\"done in %fs\" % (time() - t0))\n",
    "print(\"n_samples: %d, n_features: %d\" % X.shape)\n",
    "print()\n",
    "\n",
    "# #############################################################################\n",
    "# Do the actual clustering\n",
    "\n",
    "\n",
    "km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1,                \n",
    "            verbose=True)\n",
    "\n",
    "print(\"Clustering sparse data with %s\" % km)\n",
    "t0 = time()\n",
    "km.fit(X)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(X, km.labels_, sample_size=1000))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i, end='')\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind], end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
